import streamlit as st
import pandas as pd
import os
import gdown
import plotly.express as px
from annotated_text import annotated_text

os.environ["CUDA_VISIBLE_DEVICES"] = "-1"

st.set_page_config(page_title="Student hara-kiri", page_icon="ðŸ‘¨â€ðŸŽ“")

# --- ÐÐ°Ð»Ð°ÑˆÑ‚ÑƒÐ²Ð°Ð½Ð½Ñ ÑˆÐ»ÑÑ…Ñ–Ð² Ñ‚Ð° ID ---
MODELS_DIR = './models'
os.makedirs(MODELS_DIR, exist_ok=True)

# !!! Ð£Ð’ÐÐ“Ð: Ð—ÐÐœÐ†ÐÐ†Ð¢Ð¬ Ð¦Ð† PLACEHOLDER ID ÐÐ Ð Ð•ÐÐ›Ð¬ÐÐ† ID Ð’ÐÐ¨Ð˜Ð¥ Ð¤ÐÐ™Ð›Ð†Ð’ !!!
MODEL_IDS = {
    # SVM/TF-IDF
    "svm_model": "1_O8mtsgJipuCgqrW1yBoJBUUEYCiUXsx", 
    "tfidf_vectorizer": "1HBpIvydoh6slZKwrX4o9hsinGPydwkIi", 
    # BERT
    "bert_binary": "1D8wp3sOVV9Ri5BUG26IGVSZoSZlvjobD", 
    "bert_multiclass": "1GhTr-2ghquSTWdha96s7JJWegx2yoo2t", 
    # CNN/GLTR (SavedModel components)
    "cnn_saved_model_pb": "1e8ApGwBSC985I0eTjr_jhWMhA1amtEJi",
    "cnn_keras_metadata_pb": "19VuA-EkD-i7h-PoaoELa2lgXMdQGhQ_x",
    "cnn_variables_index": "1ExpVHEL2yan-RxsABYCEVltPwDNHiiH6", 
    "cnn_variables_data_00000": "1Pq9meCh5Q0K1HLqLRLMDHUCkDCPjmjLI",
}

# Ð¨Ð»ÑÑ…Ð¸ Ð´Ð¾ Ñ„Ð°Ð¹Ð»Ñ–Ð²
PATHS = {
    "svm_model": os.path.join(MODELS_DIR, "svm_linear_model_90000_features_probability.pkl"),
    "tfidf_vectorizer": os.path.join(MODELS_DIR, "tfidf_vectorizer_90000_features.pkl"),
    "bert_binary": os.path.join(MODELS_DIR, "model_bertbase_updated.pt"),
    "bert_multiclass": os.path.join(MODELS_DIR, "model_multiclass.pt"),
    
    # Keras SavedModel Ð²Ð¸Ð¼Ð°Ð³Ð°Ñ” Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ñ–Ñ—. ÐœÐ¸ ÑÑ‚Ð²Ð¾Ñ€ÑŽÑ”Ð¼Ð¾ Ð¿Ð¾Ñ‚Ñ€Ñ–Ð±Ð½Ñƒ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ.
    "cnn_model_dir": os.path.join(MODELS_DIR, "model_autokeras_gltr_trials_8"),
    "cnn_variables_dir": os.path.join(MODELS_DIR, "model_autokeras_gltr_trials_8", "variables"),

    "cnn_saved_model_pb": os.path.join(MODELS_DIR, "model_autokeras_gltr_trials_8", "saved_model.pb"),
    "cnn_keras_metadata_pb": os.path.join(MODELS_DIR, "model_autokeras_gltr_trials_8", "keras_metadata.pb"),
    "cnn_variables_index": os.path.join(MODELS_DIR, "model_autokeras_gltr_trials_8", "variables", "variables.index"),
    "cnn_variables_data_00000": os.path.join(MODELS_DIR, "model_autokeras_gltr_trials_8", "variables", "variables.data-00000-of-00001"), 
    # ÐŸÑ€Ð¸Ð¼Ñ–Ñ‚ÐºÐ°: Ð’ÑÑ– Ñ–Ð½ÑˆÑ– Ñ„Ð°Ð¹Ð»Ð¸ variables.data-***** Ð¿Ð¾Ñ‚Ñ€Ñ–Ð±Ð½Ð¾ Ð·Ð°Ð²Ð°Ð½Ñ‚Ð°Ð¶Ð¸Ñ‚Ð¸ Ð°Ð½Ð°Ð»Ð¾Ð³Ñ–Ñ‡Ð½Ð¾!
    # Ð¯ÐºÑ‰Ð¾ Ñƒ Ð²Ð°Ñ Ñ” Ð±Ñ–Ð»ÑŒÑˆÐµ .data Ñ„Ð°Ð¹Ð»Ñ–Ð², Ð´Ð¾Ð´Ð°Ð¹Ñ‚Ðµ Ñ—Ñ… ID Ð² MODEL_IDS Ñ‚Ð° PATHS Ñ– Ñ†Ð¸ÐºÐ» Ð·Ð°Ð²Ð°Ð½Ñ‚Ð°Ð¶ÐµÐ½Ð½Ñ.
}

# Ð¡Ñ‚Ð²Ð¾Ñ€ÐµÐ½Ð½Ñ Ð½ÐµÐ¾Ð±Ñ…Ñ–Ð´Ð½Ð¸Ñ… Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ñ–Ð¹ Ð´Ð»Ñ Keras SavedModel
os.makedirs(PATHS["cnn_variables_dir"], exist_ok=True)


@st.cache_resource(show_spinner=False)
def download_all_files(model_ids, paths):
    """Ð—Ð°Ð²Ð°Ð½Ñ‚Ð°Ð¶ÑƒÑ” Ð²ÑÑ– Ñ„Ð°Ð¹Ð»Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð· Google Drive, ÑÐºÑ‰Ð¾ Ð²Ð¾Ð½Ð¸ Ð²Ñ–Ð´ÑÑƒÑ‚Ð½Ñ–."""
    
    download_statuses = {}
    
    for key, file_path in paths.items():
        if key.startswith("cnn_") or key.startswith("bert") or key.startswith("svm") or key.startswith("tfidf"):
            
            # ÐŸÑ€Ð¾Ð¿ÑƒÑÐºÐ°Ñ”Ð¼Ð¾, ÑÐºÑ‰Ð¾ Ñ†Ðµ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ñ–Ñ
            if key.endswith("_dir"):
                continue

            # ÐŸÐµÑ€ÐµÐ²Ñ–Ñ€ÑÑ”Ð¼Ð¾, Ñ‡Ð¸ Ñ–ÑÐ½ÑƒÑ” Ñ„Ð°Ð¹Ð»
            if os.path.exists(file_path):
                download_statuses[key] = f"âœ… {os.path.basename(file_path)} already exists."
                continue
            
            # ÐŸÐµÑ€ÐµÐ²Ñ–Ñ€ÑÑ”Ð¼Ð¾, Ñ‡Ð¸ Ð²ÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ð¹ ID
            if key not in model_ids or "Ð’ÐÐ¨_ID" in model_ids[key]:
                download_statuses[key] = f"âš ï¸ {os.path.basename(file_path)} ID is missing/default."
                continue

            # Ð—Ð°Ð²Ð°Ð½Ñ‚Ð°Ð¶ÐµÐ½Ð½Ñ Ñ„Ð°Ð¹Ð»Ñƒ
            file_id = model_ids[key]
            try:
                gdown.download(f'https://drive.google.com/uc?id={file_id}', file_path, quiet=True)
                download_statuses[key] = f"âœ… {os.path.basename(file_path)} downloaded successfully."
            except Exception as e:
                download_statuses[key] = f"âŒ ERROR downloading {os.path.basename(file_path)}: {e}. Check ID and permissions."

    return download_statuses

# --- Ð¤ÐÐ—Ð 1: Ð—ÐÐ’ÐÐÐ¢ÐÐ–Ð•ÐÐÐ¯ Ð¤ÐÐ™Ð›Ð†Ð’ ÐŸÐ Ð˜ Ð¡Ð¢ÐÐ Ð¢Ð† Ð—ÐÐ¡Ð¢ÐžÐ¡Ð£ÐÐšÐ£ ---
with st.spinner("ÐŸÐµÑ€ÐµÐ²Ñ–Ñ€ÐºÐ° Ñ‚Ð° Ð·Ð°Ð²Ð°Ð½Ñ‚Ð°Ð¶ÐµÐ½Ð½Ñ Ñ„Ð°Ð¹Ð»Ñ–Ð² Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹..."):
    # Ð¦ÐµÐ¹ Ð²Ð¸ÐºÐ»Ð¸Ðº ÐºÐµÑˆÑƒÑ”Ñ‚ÑŒÑÑ Streamlit, Ñ‚Ð¾Ð¼Ñƒ Ð²Ð¸ÐºÐ¾Ð½ÑƒÑ”Ñ‚ÑŒÑÑ Ð»Ð¸ÑˆÐµ Ð¾Ð´Ð¸Ð½ Ñ€Ð°Ð·
    download_statuses = download_all_files(MODEL_IDS, PATHS)

# Ð’Ð¸Ð²Ð¾Ð´Ð¸Ð¼Ð¾ ÑÑ‚Ð°Ñ‚ÑƒÑ, Ñ‰Ð¾Ð± ÐºÐ¾Ñ€Ð¸ÑÑ‚ÑƒÐ²Ð°Ñ‡ Ð±Ð°Ñ‡Ð¸Ð², Ñ‡Ð¸ Ð²ÑÐµ Ð·Ð°Ð²Ð°Ð½Ñ‚Ð°Ð¶Ð¸Ð»Ð¾ÑÑ
with st.expander("Ð¡Ñ‚Ð°Ñ‚ÑƒÑ Ð·Ð°Ð²Ð°Ð½Ñ‚Ð°Ð¶ÐµÐ½Ð½Ñ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹"):
    for status in download_statuses.values():
        st.caption(status)


# Ð’Ð¡Ð¢ÐÐÐžÐ’Ð›Ð•ÐÐÐ¯ Ð¨Ð›Ð¯Ð¥Ð†Ð’ Ð£ Ð¡Ð•Ð¡Ð†Ð® Ð”Ð›Ð¯ UTILS
st.session_state['SVM_MODEL_PATH'] = PATHS["svm_model"]
st.session_state['SVM_VECTORIZER_PATH'] = PATHS["tfidf_vectorizer"]
st.session_state['BERT_MODEL_PATH'] = PATHS["bert_binary"]
st.session_state['BERT_MULTICLASS_PATH'] = PATHS["bert_multiclass"]
# ÐŸÐµÑ€ÐµÐ´Ð°Ñ”Ð¼Ð¾ ÑˆÐ»ÑÑ… Ð´Ð¾ Ð±Ð°Ñ‚ÑŒÐºÑ–Ð²ÑÑŒÐºÐ¾Ñ— Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ñ–Ñ— SavedModel
st.session_state['CNN_MODEL_PATH'] = PATHS["cnn_model_dir"] 
# ÐŸÑ€Ð¸Ð¼Ñ–Ñ‚ÐºÐ°: Ð¯ÐºÑ‰Ð¾ Ñ…Ð¾Ñ‡Ð° Ð± Ð¾Ð´Ð¸Ð½ ÐºÑ€Ð¸Ñ‚Ð¸Ñ‡Ð½Ð¸Ð¹ Ñ„Ð°Ð¹Ð» CNN Ð½Ðµ Ð·Ð°Ð²Ð°Ð½Ñ‚Ð°Ð¶Ð¸Ð²ÑÑ, Keras Ð²Ð¸ÐºÐ»Ð¸Ñ‡Ðµ Ð¿Ð¾Ð¼Ð¸Ð»ÐºÑƒ
# Ð¿Ñ€Ð¸ ÑÐ¿Ñ€Ð¾Ð±Ñ– Ð·Ð°Ð²Ð°Ð½Ñ‚Ð°Ð¶Ð¸Ñ‚Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ñƒ utils.initialize_models().


# --- UI: Ð¨Ð°Ð±Ð»Ð¾Ð½ Student hara-kiri ---

# Ð¤Ð°Ð¹Ð» constants.py Ð¿Ð¾Ñ‚Ñ€Ñ–Ð±ÐµÐ½ Ð´Ð»Ñ Ñ€Ð¾Ð±Ð¾Ñ‚Ð¸, ÑÑ‚Ð²Ð¾Ñ€ÑŽÑ”Ð¼Ð¾ Ð¹Ð¾Ð³Ð¾ Ð· mock-Ð´Ð°Ð½Ð¸Ð¼Ð¸
try:
    import constants
except ImportError:
    st.error("Missing `constants.py` file. Creating a placeholder.")
    class Constants:
        introduction_text = "Ð›Ð°ÑÐºÐ°Ð²Ð¾ Ð¿Ñ€Ð¾ÑÐ¸Ð¼Ð¾ Ð´Ð¾ Student hara-kiri - Ð±Ð°Ð³Ð°Ñ‚Ð¾Ð¼Ð¾Ð´ÐµÐ»ÑŒÐ½Ð¾Ñ— ÑÐ¸ÑÑ‚ÐµÐ¼Ð¸ Ð²Ð¸ÑÐ²Ð»ÐµÐ½Ð½Ñ Ñ‚ÐµÐºÑÑ‚Ñƒ, Ð·Ð³ÐµÐ½ÐµÑ€Ð¾Ð²Ð°Ð½Ð¾Ð³Ð¾ Ð¨Ð†."
        version_info = "ÐžÐ±ÐµÑ€Ñ–Ñ‚ÑŒ, Ñ‡Ð¸ Ð²Ð¸ Ð°Ð½Ð°Ð»Ñ–Ð·ÑƒÑ”Ñ‚Ðµ Ð´Ð¾Ð²Ð³Ðµ ÐµÑÐµ, Ñ‡Ð¸ ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¸Ð¹ Ñ‚ÐµÐºÑÑ‚."
        threshold_info = "ÐŸÐ¾Ñ€Ñ–Ð³ Ð¹Ð¼Ð¾Ð²Ñ–Ñ€Ð½Ð¾ÑÑ‚Ñ–, Ð²Ð¸Ñ‰Ðµ ÑÐºÐ¾Ð³Ð¾ Ð¿Ð°Ñ€Ð°Ð³Ñ€Ð°Ñ„ Ð²Ð²Ð°Ð¶Ð°Ñ”Ñ‚ÑŒÑÑ AI-Ð³ÐµÐ½ÐµÑ€Ð¾Ð²Ð°Ð½Ð¸Ð¼."
        paraphrase_checker_info = "Ð£Ð²Ñ–Ð¼ÐºÐ½Ñ–Ñ‚ÑŒ, Ñ‰Ð¾Ð± Ð¿ÐµÑ€ÐµÐ²Ñ–Ñ€Ð¸Ñ‚Ð¸, Ñ‡Ð¸ Ð±ÑƒÐ² AI-Ð³ÐµÐ½ÐµÑ€Ð¾Ð²Ð°Ð½Ð¸Ð¹ Ñ‚ÐµÐºÑÑ‚ Ð¿Ð°Ñ€Ð°Ñ„Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð¸Ð¹."
        paraphrase_threshold_info = "ÐŸÐ¾Ñ€Ñ–Ð³ Ð´Ð»Ñ Ð²Ð¸Ð·Ð½Ð°Ñ‡ÐµÐ½Ð½Ñ, Ñ‡Ð¸ Ñ” Ñ‚ÐµÐºÑÑ‚ Ð¿Ð°Ñ€Ð°Ñ„Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð¸Ð¼."
        explanation_info = "Ð“ÐµÐ½ÐµÑ€ÑƒÐ²Ð°Ñ‚Ð¸ Ð¿Ð¾ÑÑÐ½ÐµÐ½Ð½Ñ, ÑÐºÑ– ÑÐ»Ð¾Ð²Ð° Ð½Ð°Ð¹Ð±Ñ–Ð»ÑŒÑˆÐµ Ð²Ð¿Ð»Ð¸Ð½ÑƒÐ»Ð¸ Ð½Ð° ÐºÐ»Ð°ÑÐ¸Ñ„Ñ–ÐºÐ°Ñ†Ñ–ÑŽ (Ð¿Ð¾Ñ‚Ñ€Ñ–Ð±Ð½Ð° Ð¼Ð¾Ð´ÐµÐ»ÑŒ SVM)."
    constants = Constants()

st.title("ðŸ‘¨â€ðŸŽ“ Student hara-kiri")

versions = ["Essay", "Short-Text"]

# layout
intro = st.container()
select_box_col, space = st.columns(2)
version = select_box_col.selectbox("Variation", versions, help=constants.version_info)
st.write("---")
predictor = st.container()
text_annotation = st.container()
chart = st.expander("Probability breakdown by each model")
explanability = st.expander("Explanability")

# Add introduction
with intro:
    st.markdown(constants.introduction_text)

# Set up the predictor layout
threshold_col, space, generate_explanation_col = predictor.columns(3)
text_to_check = predictor.text_area("Text to analyze", height=300)
check_col, reset_col = predictor.columns(2)
ai_score, paraphrased_score = predictor.columns(2)

threshold = threshold_col.slider(
    "Threshold", 0.0, 1.0, 0.5, 0.05, help=constants.threshold_info
)

check_paraphrase = generate_explanation_col.checkbox(
    "Check if text is paraphrased", help=constants.paraphrase_checker_info
)
if check_paraphrase:
    paraphrase_threshold = generate_explanation_col.slider(
        "Paraphase threshold",
        0.0,
        1.0,
        0.6,
        0.05,
        help=constants.paraphrase_threshold_info,
        key="paraphrase",
    )

generate_explanation = generate_explanation_col.checkbox(
    "Generate explanation", help=constants.explanation_info
)
if generate_explanation:
    model_selection = "SVM"
    number_of_features = generate_explanation_col.slider(
        "Number of features",
        10,
        100,
        20,
        1,
        key="explanation",
        help="Number of features to show in the explanation, the more features, the longer it takes to generate the explanation",
    )

# variable to check if the text is written by AI
written_by_ai = False
no_cyrillic = False
button_pressed = check_col.button(
    "Check if written by AI", disabled=len(text_to_check) == 0, type="primary"
)

# --- Ð›Ð†ÐÐ˜Ð’Ð• Ð—ÐÐ’ÐÐÐ¢ÐÐ–Ð•ÐÐÐ¯ ---
if button_pressed:
    with st.spinner("Initializing models and libraries... (First run may take a while)"):
        # Ð†Ð¼Ð¿Ð¾Ñ€Ñ‚ utils Ð¿Ð¾Ð²Ð¸Ð½ÐµÐ½ Ð±ÑƒÑ‚Ð¸ Ñ‚ÑƒÑ‚, Ñ‰Ð¾Ð± Ð´Ð¾Ð·Ð²Ð¾Ð»Ð¸Ñ‚Ð¸ Ð·Ð°Ð²Ð°Ð½Ñ‚Ð°Ð¶ÐµÐ½Ð½ÑŽ Ð²Ð¸ÐºÐ¾Ð½Ð°Ñ‚Ð¸ÑÑ Ð¿ÐµÑ€ÐµÐ´ Ñ–Ð½Ñ–Ñ†Ñ–Ð°Ð»Ñ–Ð·Ð°Ñ†Ñ–Ñ”ÑŽ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹
        import utils 
    
    # Ð¢ÐµÐ¿ÐµÑ€ utils Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¸Ð¹, Ñ– Ð¼Ð¸ Ð¼Ð¾Ð¶ÐµÐ¼Ð¾ Ð¹Ð¾Ð³Ð¾ Ð²Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÐ²Ð°Ñ‚Ð¸
    no_cyrillic = not utils.has_cyrillic(text_to_check)


if button_pressed and no_cyrillic and version == versions[0]:
    # utils Ð²Ð¶Ðµ Ñ–Ð¼Ð¿Ð¾Ñ€Ñ‚Ð¾Ð²Ð°Ð½Ð¾ Ð²Ð¸Ñ‰Ðµ
    with st.spinner("Predicting..."):
        # check if the text is written by AI
        written_by_ai, scores, splitted_text = utils.check_if_ai(
            text_to_check, threshold
        )

        if check_paraphrase:
            is_paraphrased = utils.check_if_paraphrased(text_to_check)

        text_annotation.header("Text analysis")
        # generate the annotated text
        if not check_paraphrase:
            annotated_text_data = utils.generate_annotated_text(
                splitted_text, written_by_ai
            )
            with text_annotation:
                annotated_text(*annotated_text_data)

        # generate the chart
        df = pd.DataFrame.from_dict(scores["Average"], orient="index")
        fig = px.bar(
            df,
            orientation="h",
            labels={"index": "Model", "value": "Probability"},
            pattern_shape=df.index,
            color=df.index,
        )
        chart.plotly_chart(fig, use_container_width=True)

        # calculate the ai percentage
        ai_percentage = utils.check_ai_percentage(written_by_ai)
        ai_score.metric(
            label="AI",
            value=str(ai_percentage * 100)[:4] + "%",
            help="The percentage of the entire text that is written by AI",
        )

    # classify the text based on how many paragraphs are written by AI
    if ai_percentage > 0.8:
        ai_score.warning("The text is highly likely written by AI")
    elif ai_percentage > 0.6:
        ai_score.warning("The text is likely written by AI")
    elif ai_percentage > 0.4:
        ai_score.info("The text is may be written by AI")
    elif ai_percentage > 0.2:
        ai_score.success("The text is likely written by a human")
    else:
        ai_score.success("The text is most likely written by a human")

    if check_paraphrase:
        # generate the paraphrasing score
        paraphrasing_score = utils.check_if_paraphrased_percentage(
            is_paraphrased, paraphrase_threshold
        )

        annotated_text_data = utils.generate_annotated_text(
            splitted_text, written_by_ai, is_paraphrased, paraphrase_threshold
        )
        with text_annotation:
            annotated_text(*annotated_text_data)
        text_annotation.caption(
            "Legend: A red dotted border indicates that the text is paraphrased"
        )
        paraphrased_score.metric(
            label="Paraphrased",
            value=f"{paraphrasing_score*100}"[:4] + "%"
            if ai_percentage > 0.4
            else "N/A",
            help="The percentage of the entire text that is written by AI and paraphrased",
        )
        if ai_percentage > 0.4:
            # classify the text based on how many paragraphs are AI paraphrased
            if paraphrasing_score > 0.8:
                paraphrased_score.warning(
                    "The text is highly likely written by AI and paraphrased"
                )
            elif paraphrasing_score > 0.6:
                paraphrased_score.warning(
                    "The text is likely written by AI and paraphrased"
                )
            elif paraphrasing_score > 0.4:
                paraphrased_score.info(
                    "The text is may be written by AI and paraphrased"
                )
            elif paraphrasing_score > 0.2:
                paraphrased_score.success(
                    "The text is unlikely written by AI and paraphrased"
                )
        else:
            paraphrased_score.success(
                "The text is highly unlikely written by AI and paraphrased"
            )

    if generate_explanation:
        with st.spinner("Generating explanations..."):
            html = utils.get_explaination(
                text_to_check, number_of_features, model_selection
            )
            explanability._html(html, height=number_of_features * 45)

elif button_pressed and no_cyrillic and version == versions[1]:
    # utils Ð²Ð¶Ðµ Ñ–Ð¼Ð¿Ð¾Ñ€Ñ‚Ð¾Ð²Ð°Ð½Ð¾ Ð²Ð¸Ñ‰Ðµ
    with st.spinner("Predicting..."):
        # check if the text is written by AI
        written_by_ai, scores = utils.check_if_ai_short_text(text_to_check, threshold)

        if check_paraphrase:
            is_paraphrased = utils.check_if_paraphrased_short_text(text_to_check)

        # generate the chart
        df = pd.DataFrame.from_dict(scores, orient="index")
        fig = px.bar(
            df,
            orientation="h",
            labels={"index": "Model", "value": "Probability"},
            pattern_shape=df.index,
            color=df.index,
        )
        chart.plotly_chart(fig, use_container_width=True)

        # calculate the ai percentage
        ai_percentage = 1 if written_by_ai == "AI" else 0
        ai_score.metric(
            label="AI",
            value=str(ai_percentage * 100)[:4] + "%",
            help="The percentage of the entire text that is written by AI",
        )

    # classify the text based on how many paragraphs are written by AI
    if ai_percentage > 0.8:
        ai_score.warning("The text is highly likely written by AI")
    elif ai_percentage > 0.6:
        ai_score.warning("The text is likely written by AI")
    elif ai_percentage > 0.4:
        ai_score.info("The text is may be written by AI")
    elif ai_percentage > 0.2:
        ai_score.success("The text is likely written by a human")
    else:
        ai_score.success("The text is most likely written by a human")

    if check_paraphrase:
        # generate the paraphrasing score
        paraphrasing_score = is_paraphrased[2]
        paraphrased_score.metric(
            label="Paraphrased",
            value=f"{paraphrasing_score*100}"[:4] + "%"
            if ai_percentage >= 0.4
            else "N/A",
            help="The percentage of the entire text that is written by AI and paraphrased",
        )
        # only show the paraphrasing score if the text is written by AI
        if ai_percentage >= 0.4:
            # classify the text based on how many paragraphs are AI paraphrased
            if paraphrasing_score > 0.8:
                paraphrased_score.warning(
                    "The text is highly likely written by AI and paraphrased"
                )
            elif paraphrasing_score > 0.6:
                paraphrased_score.warning(
                    "The text is likely written by AI and paraphrased"
                )
            elif paraphrasing_score > 0.4:
                paraphrased_score.info(
                    "The text is may be written by AI and paraphrased"
                )
            elif paraphrasing_score > 0.2:
                paraphrased_score.success(
                    "The text is unlikely written by AI and paraphrased"
                )
        else:
            paraphrased_score.success(
                "The text is highly unlikely written by AI and paraphrased"
            )

    if generate_explanation:
        with st.spinner("Generating explanations..."):
            html = utils.get_explaination(
                text_to_check, number_of_features, model_selection
            )
            explanability._html(html, height=number_of_features * 45)

elif button_pressed and not no_cyrillic:
    st.error("The text contains cyrillic characters, which is not supported by Student hara-kiri")

if reset_col.button("Reset"):
    written_by_ai = False
    button_pressed = False
    no_cyrillic = False